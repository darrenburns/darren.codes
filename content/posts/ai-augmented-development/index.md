+++
title = 'AI Augmented Development'
date = 2025-01-07T21:33:52Z
draft = true
author = "Darren Burns"
tags = ['ai', 'llm', 'software']
+++

I've noticed a bunch of comments across social media from folk who seem unsure about how AI (specifically LLMs) are useful in software development.
Often, the comment is along the lines of "I've tried adding LLMs to my workflow but they're always making mistakes" or "LLMs just don't understand the broader context of my codebase".
I totally agree - LLMs do make a bunch of mistakes if you supply them with low context problems, or lack intuition on where their capabilities end.

I've been using Cursor almost daily for almost 6 months now, and find it incredibly helpful.
I have however found myself quickly defining personal "rules" to follow when working alongside LLMs.
These rules are mostly a result of repeatedly hitting issues, and refining my mental model on where LLMs can effectively sit in the engineering process.
Which gaps do they fill, and which gaps are still better left to humans?

## Localised refactoring

<!-- TODO: Add video. -->
